
[{"content":" \u003c!DOCTYPE html\u003e MediaPipe HandLandmarker Task for web About The theremin was invented in 1928 by Leon Thermin. It is one of the earliest electronic instruments, and one of the few instruments played without physical contact. Click here to learn more about this project. It is still a work in progress, so more features will be added.\nInstructions 1. Click the toggle above to enable your webcam.\n2. Bring hand(s) into frame.\n3. The hand nearest to the vertical antenna controls the pitch (closer = higher pitch). The hand nearest to the horizontal anetanna controls the volume (closer = lower volume). If you have one hand in the frame, only the pitch will be controlled.\nCredits \"animal company - theremin\" (https://skfb.ly/puLnZ) by Romeo rios vr is licensed under Creative Commons Attribution (http://creativecommons.org/licenses/by/4.0/). Photo by Geoffroy Berlioz on Unsplash Icon by Raj Dev on freeicons.io\nColors: swiss style color picker\n","date":"25 July 2025","externalUrl":null,"permalink":"/projects/theremin_demo/","section":"Projects","summary":"A theremin you can play in the browser by tracking your hands through the webcam.","title":"Hand-Tracking Theremin Demo","type":"projects"},{"content":"This article is a work in progress!\nIntroduction # I recently came across the MediaPipe Hand Landmarker computer vision model by Google AI Edge. This is a hand-detection model which detects 21 key points of a hand in an image, and it has a fast enough inference time on a GPU to be used for real-time tracking.\nExample of a detected hand from the docs.\nDiagram of the 21 hand landmarks from the docs.\nAfter trying the demo, I knew I wanted to use the model in a quick, fun project, however I wanted to do something a bit more original than a \u0026ldquo;gesture-controlled mouse pointer\u0026rdquo;. The first thing that came to mind was\u0026hellip; a virtual theremin.\nQuick historical tangent: the theremin is a musical instrument invented in 1920 by Leon Theremin (Lev Termen), a Soviet engineer. The instrument works by \u0026hellip; and is one of the few instruments played without physical contact.\nThis latter is the main reason why I decided to go with the theremin and not another instrument \u0026ndash; it is played without physical contact, so waving your hands in-front of the webcam would make for a failry realistic experience (at least more realistic than pretending to play the drums or guitar in thin air).\nDespite what I said about originality earlier, I\u0026rsquo;m not the first person to think of this project. After a quick web search, I found https://theremin.app/ \u0026ndash; a beautifully designed site that allows you to play the theremin by either using your mouse or by tracking your hands through the webcam. However, I encountered a few issues with the theremin.app implementation I wanted to fix in my version.\nFirst, I could not get finger-tracking to work. Second, the pitch changes are very jumpy, resulting in stoccato notes instead of the smooth, continuous sound of a real theremin. Both issues take away from the realism, which I wanted to fix.\nI also wanted to make this project an opportunity to learn more CSS and JavaScript, as well as learn more about synthesizers. Below is a step-by-step description of how you can recreate the same project. The full code on GitHub will be linked at the end.\nA minimal implementation is as follows:\nEnable webcamm and detect hand(s) in each frame. Display the hands in 3D from a first-person perspective on a canvas with a 2D or 3D theremin. The hands should control the pitch and volume by moving towards/away from corresponding antennas. Start Local HTTP Server # First, create a root directory for your project and create your .html and .js files in it. If you want to spice up your project with beautiful CSS, you can create a separate .css file, but since this is going to be a very minimal example, I\u0026rsquo;m not going to bother.\ntheremin_project/ ├── index.html └── script.js In the root directory, run the follwing command in the terminal to launch a local http server for your project.\npython3 -m http.server 8000 You can now go to http://localhost:8000/ and view index.html.\nSet Up Hand Landmarker # We will follow the Hand Landmarker code example for the web.\nPaste the following into index.html.\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34; \u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Hand-Tracking Theremin\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; .invisible { opacity: 0.2; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hand-Tracking Theremin\u0026lt;/h1\u0026gt; \u0026lt;section id=\u0026#34;demos\u0026#34; class=\u0026#34;invisible\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;liveView\u0026#34; class=\u0026#34;videoView\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;webcamButton\u0026#34;\u0026gt; \u0026lt;span \u0026gt;ENABLE WEBCAM\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;div style=\u0026#34;position: relative;\u0026#34;\u0026gt; \u0026lt;video id=\u0026#34;webcam\u0026#34; style=\u0026#34;position: abso\u0026#34; autoplay playsinline\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;canvas class=\u0026#34;output_canvas\u0026#34; id=\u0026#34;output_canvas\u0026#34; style=\u0026#34;position: absolute; left: 0px; top: 0px;\u0026#34;\u0026gt;\u0026lt;/canvas\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;!-- partial --\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;./script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; In the browser, your page should look like below.\nMake sure that you are importing the MediaPipe scripts\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; before importing your own script.js:\n\u0026lt;!-- partial --\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;./script.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; I will explain the .invisible CSS in a moment.\nNow paste the following code into script.js.\nimport { HandLandmarker, FilesetResolver } from \u0026#34;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0\u0026#34;; const demosSection = document.getElementById(\u0026#34;demos\u0026#34;); let handLandmarker = undefined; let runningMode = \u0026#34;IMAGE\u0026#34;; let enableWebcamButton; let webcamRunning = false; let rafID = null; // Before we can use HandLandmarker class we must wait for it to finish // loading. Machine Learning models can be large and take a moment to // get everything needed to run. const createHandLandmarker = async () =\u0026gt; { const vision = await FilesetResolver.forVisionTasks(\u0026#34;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm\u0026#34;); handLandmarker = await HandLandmarker.createFromOptions(vision, { baseOptions: { modelAssetPath: `https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task`, delegate: \u0026#34;GPU\u0026#34; }, runningMode: runningMode, numHands: 2 }); demosSection.classList.remove(\u0026#34;invisible\u0026#34;); }; createHandLandmarker(); const video = document.getElementById(\u0026#34;webcam\u0026#34;); const canvasElement = document.getElementById(\u0026#34;output_canvas\u0026#34;); const canvasCtx = canvasElement.getContext(\u0026#34;2d\u0026#34;); // Check if webcam access is supported. const hasGetUserMedia = () =\u0026gt; { var _a; return !!((_a = navigator.mediaDevices) === null || _a === void 0 ? void 0 : _a.getUserMedia); }; // If webcam supported, add event listener to button for when user // wants to activate it. if (hasGetUserMedia()) { enableWebcamButton = document.getElementById(\u0026#34;webcamButton\u0026#34;); enableWebcamButton.addEventListener(\u0026#34;click\u0026#34;, event =\u0026gt; { if (!webcamRunning) { // turn webcam off; enableCam(); } else { // turn webcam off disableCam(); } }); } else { console.warn(\u0026#34;getUserMedia() is not supported by your browser\u0026#34;); } // Enable the live webcam view and start detection. async function enableCam(event) { if (!handLandmarker) { console.log(\u0026#34;Wait! objectDetector not loaded yet.\u0026#34;); return; } webcamRunning = true; enableWebcamButton.innerText = \u0026#34;DISABLE WEBCAM\u0026#34;; // getUsermedia parameters. const constraints = { video: true }; // Activate the webcam stream. await handLandmarker.setOptions({ runningMode: \u0026#34;VIDEO\u0026#34; }); navigator.mediaDevices.getUserMedia(constraints).then((stream) =\u0026gt; { video.srcObject = stream; video.addEventListener(\u0026#34;loadeddata\u0026#34;, predictWebcam); }); } // Disable webcam function disableCam() { if (video.srcObject) { video.srcObject.getTracks().forEach(track =\u0026gt; track.stop()); video.srcObject = null; if (rafID) { cancelAnimationFrame(rafID); rafID = null; } //cancelAnimationFrame(-1); video.removeEventListener(\u0026#34;loadeddata\u0026#34;, predictWebcam) } canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height); webcamRunning = false; enableWebcamButton.innerText = \u0026#34;ENABLE WEBCAM\u0026#34;; } let lastVideoTime = -1; let results = undefined; console.log(video); async function predictWebcam() { canvasElement.style.width = video.videoWidth; canvasElement.style.height = video.videoHeight; canvasElement.width = video.videoWidth; canvasElement.height = video.videoHeight; // Now let\u0026#39;s start detecting the stream. let startTimeMs = performance.now(); if (lastVideoTime !== video.currentTime) { lastVideoTime = video.currentTime; results = handLandmarker.detectForVideo(video, startTimeMs); } canvasCtx.save(); canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height); if (results.landmarks) { for (const landmarks of results.landmarks) { drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: \u0026#34;#00FF00\u0026#34;, lineWidth: 5 }); drawLandmarks(canvasCtx, landmarks, { color: \u0026#34;#FF0000\u0026#34;, lineWidth: 2 }); } } canvasCtx.restore(); // Call this function again to keep predicting when the browser is ready. if (webcamRunning === true) { rafID = window.requestAnimationFrame(predictWebcam); } } Click the ENABLE WEBCAM button, allow webcam permissions, and you should see something like this.\nIf you click DISABLE WEBCAM, the webcam should shut down. You should be able to re-enable it without error. The code is somewhat easy to understand, but let here\u0026rsquo;s one important part:\nlet rafID = null; // ... function disableCam() { if (video.srcObject) { // ... if (rafID) { cancelAnimationFrame(rafID); rafID = null; } // ... } // ... } // ... async function predictWebcam() { // ... if (webcamRunning === true) { rafID = window.requestAnimationFrame(predictWebcam); } } When disabling the webcam, you must cancel the last frame by its ID (rafID), otherwise upon re-enabling the camera you will get this error:\nUncaught (in promise) Error: WaitUntilIdle failed: $CalculatorGraph::Run() failed in Run: Calculator::Process() for node \u0026#34;mediapipe_tasks_vision_hand_landmarker_handlandmarkergraph__mediapipe_tasks_vision_hand_detector_handdetectorgraph__mediapipe_tasks_components_processors_imagepreprocessinggraph__ImageToTensorCalculator\u0026#34; failed: RET_CHECK failure (third_party/mediapipe/calculators/tensor/image_to_tensor_utils.cc:56) roi-\u0026gt;width \u0026gt; 0 \u0026amp;\u0026amp; roi-\u0026gt;height \u0026gt; 0 ROI width and height must be \u0026gt; 0.; WaitUntilIdle failed This minimal implementation of Hand Landmarker should be enough to get you started. However, let\u0026rsquo;s take it further and have the hands display from a first-person perspective in a separate 3D canvas using Three.js.\nAdd Three.js 3D Canvas # Add the following code to the \u0026lt;head\u0026gt; of index.html:\n\u0026lt;script type=\u0026#34;importmap\u0026#34;\u0026gt; { \u0026#34;imports\u0026#34;: { \u0026#34;three\u0026#34;: \u0026#34;https://cdn.jsdelivr.net/npm/three@0.152.2/build/three.module.js\u0026#34;, \u0026#34;three/addons/\u0026#34;: \u0026#34;https://cdn.jsdelivr.net/npm/three@0.152.2/examples/jsm/\u0026#34; } } \u0026lt;/script\u0026gt; Then import Three.js in script.js below your Hand Landmarker import:\nimport * as THREE from \u0026#39;three\u0026#39;; import { OrbitControls } from \u0026#39;three/addons/controls/OrbitControls.js\u0026#39;; Let\u0026rsquo;s edit index.html to create a second canvas for 3D rendering next to of the webcam stream. Edit the liveView container to add a new canvas below output_canvas:\n\u0026lt;canvas class=\u0026#34;output_canvas\u0026#34; id=\u0026#34;output_canvas\u0026#34; style=\u0026#34;position: absolute; left: 0px; top: 0px;\u0026#34;\u0026gt;\u0026lt;/canvas\u0026gt; \u0026lt;!-- added the new canvas below --\u0026gt; \u0026lt;canvas class=\u0026#34;canvas3d\u0026#34; id=\u0026#34;canvas3d\u0026#34; style=\u0026#34;position: absolute; right: 0px; top: 0px;\u0026#34;\u0026gt;\u0026lt;/canvas\u0026gt; Now let\u0026rsquo;s connected Three renderer to canvas3d. Add the following code to script.js after where you add the listener event to the ENABLE WEBCAM button:\n// ----- Three.js setup ----- // Scene const scene = new THREE.Scene(); let width = 640; // you can change this to whatever you want let height = 480; // Camera const camera = new THREE.PerspectiveCamera(60, 1, 0.01, 10); camera.position.set(0.5, 0.5, 0.5); camera.lookAt(0,0,0); camera.aspect = width/ height; camera.updateProjectionMatrix(); // Renderer const canvas3D = document.getElementById(\u0026#34;canvas3d\u0026#34;); const renderer = new THREE.WebGLRenderer({alpha: true, canvas: canvas3D}); renderer.setClearColor(0xdddddd, 1); renderer.setSize(width, height, true); // size of the helpers in world units const helperSize = 2; const gridDivisions =10; // grid const gridHelper = new THREE.GridHelper(helperSize, gridDivisions); scene.add(gridHelper); // axes const axesHelper = new THREE.AxesHelper(helperSize); scene.add(axesHelper); // OrbitControls const controls = new OrbitControls( camera, renderer.domElement ); controls.update(); You should see something like the image below. You should be able to rotate, pan and zoom the rendered scene using left-click, right-click and scroll wheel, respectively.\nIt\u0026rsquo;s later going to be important to know which axis is which, so remember:\nX: red Y: green Z: blue Render Hand Landmarks in 3D # Now we will edit predictWebcam() to display the hand landmarks in 3D. First, add these imports to the top of script.js\nimport { LineSegments2 } from \u0026#39;three/addons/lines/LineSegments2.js\u0026#39;; import { LineMaterial } from \u0026#39;three/addons/lines/LineMaterial.js\u0026#39;; import { LineSegmentsGeometry } from \u0026#39;three/addons/lines/LineSegmentsGeometry.js\u0026#39;; Next, add this code below your renderer set-up code:\n// Hands geometry const lineSegWidth = 0.01; // Hand 1 const skeletonGeom1 = new LineSegmentsGeometry(); const skeletonMat1 = new LineMaterial({ color: 0xff0000, // red color linewidth: lineSegWidth, // 0.005 world units thick worldUnits: true }); const skeletonMesh1 = new LineSegments2(skeletonGeom1, skeletonMat1); scene.add(skeletonMesh1); // Hand 2 const skeletonGeom2 = new LineSegmentsGeometry(); const skeletonMat2 = new LineMaterial({ color: 0x0000ff, // blue color linewidth: lineSegWidth, worldUnits: true }); const skeletonMesh2 = new LineSegments2(skeletonGeom2, skeletonMat2); scene.add(skeletonMesh2); // get renderer size const size = new THREE.Vector2(); renderer.getSize(size); // Set resolution to match renderer size skeletonMat1.resolution.set(size.x, size.y); skeletonMat2.resolution.set(size.x, size.y); This code creates two meshes \u0026ndash; one for each hand. For clarity, I made the first one red and the second one blue, but you can choose any color. Next we need to understand what results the Hand Landmarker actually returns. Quoting from the docs:\nLandmarks: There are 21 hand landmarks, each composed of x, y and z coordinates. The x and y coordinates are normalized to [0.0, 1.0] by the image width and height, respectively. The z coordinate represents the landmark depth, with the depth at the wrist being the origin. The smaller the value, the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x. World Landmarks: The 21 hand landmarks are also presented in world coordinates. Each landmark is composed of x, y, and z, representing real-world 3D coordinates in meters with the origin at the hand’s geometric center. HandLandmarkerResult: Handedness: Categories #0: index : 0 score : 0.98396 categoryName : Left Landmarks: Landmark #0: x : 0.638852 y : 0.671197 z : -3.41E-7 Landmark #1: x : 0.634599 y : 0.536441 z : -0.06984 ... (21 landmarks for a hand) WorldLandmarks: Landmark #0: x : 0.067485 y : 0.031084 z : 0.055223 Landmark #1: x : 0.063209 y : -0.00382 z : 0.020920 ... (21 world landmarks for a hand) After experimenting with the different co-ordinate readings, this is what we\u0026rsquo;re going to do: the hands will move in 3D locally, about the wrist (landmark #0). However globally they will move in the x-y plane only. Add these two helper functions:\nfunction getHandPoints(results, hand_index, pts){ const moving_origin = results.landmarks[hand_index][0]; const hand3D = results.worldLandmarks[hand_index]; const wrist = hand3D[0]; for (const [i, j] of HAND_CONNECTIONS) { const A = hand3D[i], B = hand3D[j]; pts.push(A.x, A.y, A.z, B.x, B.y, B.z); } } function updateHandSkeleton(geom, mesh, pts){ geom.setPositions(pts); mesh.rotation.y = Math.PI; mesh.updateMatrixWorld(); geom.computeBoundingSphere(); } Then edit predictWebcam as follows:\nasync function predictWebcam() { canvasElement.style.width = video.videoWidth; canvasElement.style.height = video.videoHeight; canvasElement.width = video.videoWidth; canvasElement.height = video.videoHeight; // Now let\u0026#39;s start detecting the stream. let startTimeMs = performance.now(); if (lastVideoTime !== video.currentTime) { lastVideoTime = video.currentTime; results = handLandmarker.detectForVideo(video, startTimeMs); } canvasCtx.save(); canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height); if (results.landmarks) { for (const landmarks of results.landmarks) { drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: \u0026#34;#00FF00\u0026#34;, lineWidth: 5 }); drawLandmarks(canvasCtx, landmarks, { color: \u0026#34;#FF0000\u0026#34;, lineWidth: 2 }); } // ----- Added this ----- skeletonMesh1.visible = true; skeletonMesh2.visible = true; const pts1 = []; const pts2 = []; if (results.landmarks.length === 1){ getHandPoints(results, 0, pts1); updateHandSkeleton(skeletonGeom1, skeletonMesh1, pts1); } else if (results.landmarks.length === 2){ getHandPoints(results, 0, pts1); getHandPoints(results, 1, pts2); updateHandSkeleton(skeletonGeom1, skeletonMesh1, pts1); updateHandSkeleton(skeletonGeom2, skeletonMesh2, pts2); } else{ // handle the situation when there are more than two hands. } } else{ skeletonMesh1.visible = false; skeletonMesh2.visible = false; } // ------------------------------- canvasCtx.restore(); // Call this function again to keep predicting when the browser is ready. if (webcamRunning === true) { rafID = window.requestAnimationFrame(predictWebcam); } } At this point, we don\u0026rsquo;t really need to draw the landmarks on the 2D canvas. You may want to remove the 2D landmark drawing code and the 2D canvas alltogether, but you don\u0026rsquo;t have to. I will remove it in this example to prevent clutter.\nYou might notice that while the two hands appear to be moving in 3D, they are fixed at the origin. This is precisely because \u0026ldquo;each landmark is composed of x, y, and z, representing real-world 3D coordinates in meters with the origin at the hand’s geometric center,\u0026rdquo; as said in the docs.\nImport Theremin 3D Model # Calculated Distances To Antennas # Add Sound With Tone.js # Conclusion # ","date":"25 July 2025","externalUrl":null,"permalink":"/posts/theremin/","section":"Posts","summary":"Build a musical instrument you can play in your browser with JavaScript.","title":"How to Make a Hand-Tracking Theremin Browser Game","type":"posts"},{"content":"","date":"25 July 2025","externalUrl":null,"permalink":"/","section":"Polina Shopina","summary":"","title":"Polina Shopina","type":"page"},{"content":"","date":"25 July 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"Hi! I’m Polina, a recent Engineering Physics graduate from the University of British Columbia. My interests include robotics, aerospace engineering, physics, and machine learning. I’ve worked on a number of cool projects, which you can check out here. My most recent project is an automated ballistic interceptor. Besides engineering, I enjoy hiking, reading and film photography.\n","date":"16 July 2025","externalUrl":null,"permalink":"/about/","section":"Polina Shopina","summary":"\u003cp\u003eHi! I’m Polina, a recent Engineering Physics graduate from the University of British Columbia. My interests include robotics, aerospace engineering, physics, and machine learning. I’ve worked on a number of cool projects, which you can check out \u003ca\n  href=\"/projects/\"\u003ehere\u003c/a\u003e. My most recent project is an automated ballistic interceptor. Besides engineering, I enjoy hiking, reading and film photography.\u003c/p\u003e","title":"About Me","type":"page"},{"content":"A showcase of my personal and academic projects.\n","date":"16 July 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"\u003cp\u003eA showcase of my personal and academic projects.\u003c/p\u003e","title":"Projects","type":"projects"},{"content":" This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n","date":"16 July 2025","externalUrl":null,"permalink":"/resume/","section":"Polina Shopina","summary":"\u003cobject data=\"resume.pdf\" type=\"application/pdf\" width=\"700px\" height=\"700px\"\u003e\n    \u003cembed src=\"resume.pdf\"\u003e\n        \u003cp\u003eThis browser does not support PDFs. Please download the PDF to view it: \u003ca href=\"resume.pdf\"\u003eDownload PDF\u003c/a\u003e.\u003c/p\u003e","title":"Resume","type":"page"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/agriculture/","section":"Tags","summary":"","title":"Agriculture","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":" Top Skills Used # 3-axis gantry control and automation Computer vision Python GUI application development Working in a startup environment Project Overview # The Autonomous Seed Interrogator was my first capstone project (ENPH 459). Working in a team of five, we developed a test-bench prototype for the Vancouver-based startup Insporos. The device automates the examination (\u0026ldquo;interrogation\u0026rdquo;) of various vegetable seeds using RGB imaging and other sensors to detect diseases and abnormalities. More information about the project’s purpose and scope can be found in our project fair poster:\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\nMy contributions included automating the gantry system, implementing computer vision techniques, and developing a Python GUI software to control the prototype. This project was quite challenging because it was still in the exploratory phase, and so many variables were undefined. Nevertheless, it was a rewarding learning experience becuase I got to collaborate with a startup focused on addressing a real-world agricultural issue.\nYour browser does not support the video tag. Please update your browser or [download the video](seed_interrogator.mp4). The seed interrogator in action.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/seed/","section":"Projects","summary":"Built an autonomous seed-sorting device for a Vancouver-based start-up Insporos.","title":"Autonomous Seed Interrogator","type":"projects"},{"content":" Top Skills Used # Computer vision Real-time stereo vision Predictive algorithms C++ GUI application development Motor control Introduction # I built an automated beer pong opponent for my senior year capstone project (ENPH 479) as part of a four-person team. Our objective was to design and build a system that can predict the trajectory of a ball thrown toward a “solo cup” and intercept it mid-flight by shooting pellets from an airsoft gun. The basic operation is shown in the system diagram.\nSystem diagram.\nOverview # This project had two main parts: a stereo computer vision system built with two FLIR cameras, allowing for real-time 3D trajectory estimation, and the launcher itself, actuated by two stepper encoder motors. The main challenge of this project was the integration between the two. How do the motors know where to move to based on what the cameras are seeing? This is known as the hand-eye calibration problem in robotics, and it was one of the most fun puzzles I’ve ever solved.\nThis was a very complex project and there is A LOT to talk about, so I encourage you to check out our landing-page GitHub and our main code repository for more (perhaps too much) detail. Nevertheless, here is a brief summary:\nCreated the mechanical design in CAD (OnShape) and built it using 3D-printed parts and off-the-shelf components optimized for high-torque loads. Built a high-precision stereo vision system using FLIR cameras, Spinnaker SDK, and OpenCV, controlled by a C++ Qt6 GUI application. Implemented a unique calibration to convert stereo coordinates to motor orientation coordinates, solving the hand-eye calibration problem. Trained and integrated a YOLOv8 model for object detection and accelerated image processing 10x using CUDA for real-time performance. Assembled system in real life.\nQt GUI application interface.\nYour browser does not support the video tag. Please update your browser or [download the video](Aiming.mp4). Demonstration of our launcher aiming at a static tennis ball with a laser pointer. The laser dot on the ball is difficult to see in the video, but it shows up most clearly at 0:05.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/interceptor/","section":"Projects","summary":"Built an automated ballistic interceptor that used a high-speed stereo vision system and OpenCV to predict 3D ball trajectories for interception with a projectile from a stepper‑driven BB launcher.","title":"Ballistic Interceptor","type":"projects"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/c++/","section":"Tags","summary":"","title":"C++","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/cad/","section":"Tags","summary":"","title":"CAD","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/co-op/","section":"Tags","summary":"","title":"Co-Op","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/cv/","section":"Tags","summary":"","title":"Cv","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/cython/","section":"Tags","summary":"","title":"Cython","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/gui/","section":"Tags","summary":"","title":"GUI","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/internship/","section":"Tags","summary":"","title":"Internship","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/mechanical/","section":"Tags","summary":"","title":"Mechanical","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/onshape/","section":"Tags","summary":"","title":"Onshape","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/opencv/","section":"Tags","summary":"","title":"OpenCV","type":"tags"},{"content":" Top Skills Used # SolidWorks 3D CAD and PDM Design for manufacturing SolidWorks drawings and GD\u0026amp;T FEA simulations Machine shop tools (laser engraver, mill, drill, saw) IP and vibration testing Programmable instruments control Quality control Project Overview # I worked as a mechanical engineering intern and later as a part-time employee at Orbital Research Ltd., a company specializing in satellite communication devices, such as low-noise block up/down converters and precision local oscillators.\nAt Orbital Research, I designed parts for RF receivers, transmitters, and other devices using SolidWorks. I created detailed machine shop drawings, adhering to GD\u0026amp;T standards, and communicated with the shop to ensure quality manufacturing. I also designed complex outdoor unit assemblies and produced assembly drawings for customers.\nOne highlight involved making mechanical modifications to existing devices to meet specific customer requirements. I took initiative in this project by designing the changes in SolidWorks, fabricating them using machine shop tools, and communicating with the client. The finalized prototypes were delivered on time, achieving customer satisfaction.\nBeyond design and manufacturing, I also conducted IP and vibration testing on the Orbital devices. I built a vibration testing jig and developed a user-friendly Python GUI to operate it. The setup included a vibration table, a function generator, and an oscilloscope. My software ensured that the output vibration profile matched the input profile through transfer function calibration.\nI am really grateful for the opportunities I had at Orbital Research, as they allowed me to expand my skillset beyond mechanical design and into various other areas of engineering.\nOrbital Research Ltd.\nExamples of Orbital Research products from their product page.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/orbital/","section":"Projects","summary":"Designed RF up/down converter components in SolidWorks, including machine‑shop drawings with GD\u0026amp;T. Fabricated customer‑specific mechanical modifications on time. Built and automated a vibration‑testing jig.","title":"Orbital Research Internship","type":"projects"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/physics/","section":"Tags","summary":"","title":"Physics","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/prototyping/","section":"Tags","summary":"","title":"Prototyping","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/qt/","section":"Tags","summary":"","title":"Qt","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/solidworks/","section":"Tags","summary":"","title":"Solidworks","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/testing/","section":"Tags","summary":"","title":"Testing","type":"tags"},{"content":" Top Skills Used # Onshape 3D CAD 3D printing Laser cutting Motor control Sensor integration Microcontroller programming in C Electrical circuitry and soldering Project Overview # During a summer course called Robot Summer (ENPH 253) at UBC, my team of four built a fully autonomous robot for a student competition. This was one of the most exciting and challenging projects I’ve ever undertaken.\nOur goal was to design a robot that could navigate a complex obstacle course and use a claw mechanism to collect and store “treasures” to deliver them to a designated drop-off zone. The project timeline was about six weeks, and we were limited to basic electrical components and materials. We built our own H‑bridge circuits and even manufactured our own gears.\nI designed the base platform in Onshape and integrated the CAD models from other team members. The base parts were laser‑cut from fibreboard and the robot’s two driving wheels were powered by encoder motors. I implemented differential drive to allow for tight turns throughout the obstacle course.\nThe Onshape CAD assembly I created for the robot.\nOur completed robot, nicknamed “Gluey” due to our generous use of hot glue to keep it together.\nThe treasure basket located at the back of the robot.\nThe first part of the course involved following a line of black tape. I built a circuit using IR reflectivity sensors to detect the tape, and working with another teammate, we implemented PID control to keep the robot on the line. After several days of tuning, we achieved consistent (although slightly wobbly) line following.\nYour browser does not support the video tag. Please update your browser or [download the video](test_run.mp4). Gluey following the black tape.\nBesides PID control, I programmed the microcontrollers to integrate the robot’s various sensors and subsystems, including ultrasonic sensors for detecting treasures and obstacles, a magnetic sensor to identify “bomb” treasures with magnets inside, and the mechanical claw with four degrees of freedom.\nDue to the tight timeline, we didn’t have time to perfect our design. Our claw behaved somewhat inconsistently, and we only managed to navigate about half of the obstacle course. Nevertheless, our robot successfully collected two treasures in the first part of the course and proved to be a worthy opponent in the competition.\nYour browser does not support the video tag. Please update your browser or [download the video](competition.mp4). A run from the competition. I’m the one in the red t‑shirt.\nWhile our robot was not perfect, and there are many changes I would make to the design if I were to build it today, it provided valuable experience in robotics, teamwork, and project management.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/robotsummer/","section":"Projects","summary":"Built an autonomous differential‑drive robot built in a six‑week UBC ENPH 253 course. It uses IR, ultrasonic, and magnetic sensors with PID control to navigate an obstacle course and collect objects with a four‑DOF claw.","title":"Treasure-Collecting Robot Competition","type":"projects"},{"content":" Top Skills Used # Motor control 3D printing Python GUI application development Programmable instrument communication Electrical circuitry and soldering IoT networking Project Overview # During my internship at TRIUMF, Canada’s particle accelerator centre, I worked with the UltraCold Neutron group to develop test equipment for a neutron electric dipole moment experiment.\nI improved the design of an automated gradiometer, a device used to measure weak magnetic fields produced by potentially magnetized components. Because the experiment involves detecting extremely weak magnetic fields inside a magnetically shielded room, the smallest interference from internal components could compromise the measurement. To ensure accuracy, every component placed inside the room must be examined with the gradiometer.\nThe gradiometer consists of a conveyor belt controlled by two stepper motors and is shielded from external magnetic fields with Mu-metal. As a component travels along the conveyor belt, it passes through the Mu-metal shield, where fluxgate sensors measure its magnetic field. I automated the stepper motor calibration and developed a Python script to control programmable instruments, including a multimeter and an oscilloscope, to automate the fluxgate measurements.\nIn addition to the gradiometer, I built battery-powered temperature sensors with ESP8266 microcontrollers and temperature-measuring modules. I configured the network so the sensors would regularly transmit temperature readings and battery levels to the server for continuous monitoring. These sensors helped us verify the dependancy of fluxgate measurements on temperature before integrating them into the gradiometer.\nThe gradiometer setup.\nAssembled temperature sensors used in the experiment.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/triumf/","section":"Projects","summary":"Improved an automated Mu‑metal‑shielded gradiometer and wrote Python scripts to control multimeters and oscilloscopes for fluxgate measurements.","title":"TRIUMF Internship","type":"projects"},{"content":" Top Skills Used # SolidWorks 3D CAD Aerodynamic constraint analysis Parameter optimization using Python Team leadership and collaboration Project Overview # I was a member of the UBC AeroDesign team, which designs and builds remote-controlled aircraft for the SAE Aero Design competition. As the Advanced Class fuselage subteam lead, I designed a portion of the fuselage in SolidWorks and wrote a Python script to perform aerodynamic constraint analysis and parameter optimization. This helped us identify parameters that would maximize our score at the competition.\nMy fuselage transition region design. It creates a smooth transition between complex airfoil shapes, using carbon fibre and balsa wood.\nRender of the Advanced Class aircraft. The transition region is situated between the main fuselage and the wing.\nOutput of the Python parameter optimization script. These identified parameters would maximize our SAE competition score.\nTeam photo.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/aerodesign/","section":"Projects","summary":"Was the Advanced Class fuselage subteam lead for UBC AeroDesign; designed fuselage parts in SolidWorks and wrote a Python script to perform aerodynamic constraint analysis and optimize design parameters to maximize SAE competition score.","title":"UBC SAE AeroDesign Team","type":"projects"},{"content":" Top Skills Used # ROS, Gazebo Machine learning Computer vision Python data pipelines Data augmentation Project Overview # As part of a machine learning course ENPH 353, I worked in a team of two to train a virtual autonomous agent to navigate a 3D simulated obstacle course. The goal was to drive around the virtual environment, identify parked cars, and accurately read their licence plates using computer vision — all completely autonomously. Our code repository for this project is available on my GitHub.\nThe simulation was set up in Gazebo Gym and included parked cars along the roadside, pedestrians crossing randomly, and a moving NPC truck. Our objective was to complete the course as quickly as possible while correctly reading all eight randomly generated plates. Points were taken away for driving off the road or colliding with obstacles, so precise navigation and perception were crucial.\nAn overview of the virtual obstacle course.\nThe route our virtual car followed around the course.\nTo achieve autonomous driving, we employed imitation learning. First, we controlled the agent manually using ROS, collecting thousands of images from the agent’s cameras alongside our control inputs (turn left, turn right, or go straight). This dataset was then used to train a convolutional neural network that predicted steering commands based on the camera input. Our inspiration came from how Tesla trains its autonomous driving systems.\nYour browser does not support the video tag. Please update your browser or [download the video](driving_compatible.mp4). Processed camera feed from the agent’s cameras used for training the CNN.\nThe CNN architecture predicting the agent’s steering commands.\nWe also used computer vision techniques to detect and read the car plates. After identifying the plates, we used another neural network trained on augmented data to recognize the characters. This was challenging due to the\nblurriness of the plate images. To improve accuracy, we averaged results across multiple frames of the same plate.\nA detected plate, unprocessed.\nExtracting blue text from the plate.\nSegmenting individual symbols from the plate text.\nA processed symbol ready for input into the character‑recognition network.\nIn the end, our efforts paid off: we correctly identified all car plates during the competition. While our agent wasn’t the quickest to navigate the obstacle course, my partner and I were very happy with our precision and consistency in reading the plates. This project inspired me to study machine learning further. In the following semester, I became a teaching assistant for this course and got to learn the material at an even deeper level.\n","date":"15 July 2025","externalUrl":null,"permalink":"/projects/gazebo/","section":"Projects","summary":"Trained an agent in Gazebo to autonomously navigate a 3D obstacle course (imitation learning) and correctly read random license plates (CNN) while avoiding collisions.","title":"Virtual Autonomous Driving Robot Competition","type":"projects"},{"content":"","date":"15 July 2025","externalUrl":null,"permalink":"/tags/webgl/","section":"Tags","summary":"","title":"Webgl","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]